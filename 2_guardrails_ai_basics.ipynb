{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19af586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e7c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Guardrails AI\n",
    "%pip install guardrails-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7d4f1",
   "metadata": {},
   "source": [
    "## What we learned in Notebook 1:\n",
    "- Rule-based guardrails work but require constant maintenance\n",
    "- Regex patterns are easy to bypass with variations\n",
    "- We had to write custom logic for each risk type\n",
    "\n",
    "## Problems with Rule-Based Approach:\n",
    "- ‚ùå \"john@email.com\" gets caught, but \"john [at] email [dot] com\" doesn't\n",
    "- ‚ùå Need to maintain growing lists of patterns\n",
    "- ‚ùå Lots of false positives and false negatives\n",
    "- ‚ùå Can't handle nuanced cases\n",
    "\n",
    "## Solution: Guardrails AI\n",
    "- ‚úÖ Pre-built validators for common risks\n",
    "- ‚úÖ ML-based detection (more robust)\n",
    "- ‚úÖ Community-maintained and updated\n",
    "- ‚úÖ Easy to use and compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the configure command:\n",
    "!guardrails configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea39115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now install the validator\n",
    "!guardrails hub install hub://guardrails/detect_pii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guardrails import Guard\n",
    "from guardrails.hub import DetectPII\n",
    "\n",
    "# Create a guard with PII detection\n",
    "guard = Guard().use(\n",
    "    DetectPII(\n",
    "        pii_entities=[\"EMAIL_ADDRESS\", \"PHONE_NUMBER\"],\n",
    "        on_fail=\"exception\"   #exception\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Guard created with DetectPII validator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c6623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Text with PII (should be caught)\n",
    "text_with_pii = \"My email is john@example.com and my phone is 555-123-4567\"\n",
    "\n",
    "print(\"Testing text with PII:\")\n",
    "print(f\"Input: {text_with_pii}\\n\")\n",
    "\n",
    "try:\n",
    "    result = guard.validate(text_with_pii)\n",
    "    print(f\"‚úÖ Validation passed\")\n",
    "    print(f\"Output: {result.validated_output}\")\n",
    "except Exception as e:\n",
    "    print(f\"üö´ Validation failed!\")\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d7149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Text without PII (should pass)\n",
    "text_without_pii = \"I love learning about AI and guardrails!\"\n",
    "\n",
    "print(\"\\nTesting text without PII:\")\n",
    "print(f\"Input: {text_without_pii}\\n\")\n",
    "\n",
    "try:\n",
    "    result = guard.validate(text_without_pii)\n",
    "    print(f\"‚úÖ Validation passed\")\n",
    "    print(f\"Output: {result.validated_output}\")\n",
    "except Exception as e:\n",
    "    print(f\"üö´ Validation failed!\")\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3b1e53",
   "metadata": {},
   "source": [
    "## Why is DetectPII Better Than Our Regex?\n",
    "\n",
    "Remember our simple regex from Notebook 1? Let's see how DetectPII handles variations that would bypass regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eeb8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variations would BYPASS our simple regex from Notebook 1\n",
    "standard_pii_cases = [\n",
    "    \"Email me at john.doe@company.com\",\n",
    "    \"Call 555-123-4567 for more info\",\n",
    "    \"My phone number is (555) 123-4567\",\n",
    "    \"Contact: jane_smith@example.org\",\n",
    "    \"SSN: 123-45-6789\",  # Let's see if it catches this\n",
    "]\n",
    "\n",
    "print(\"Testing tricky PII variations:\\n\")\n",
    "for i, text in enumerate(standard_pii_cases, 1):\n",
    "    print(f\"Test {i}: {text}\")\n",
    "    try:\n",
    "        result = guard.validate(text)\n",
    "        print(f\"   ‚úÖ Passed (no PII detected)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   üö´ Caught! DetectPII found PII\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc6dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install toxic language detector\n",
    "!guardrails hub install hub://guardrails/toxic_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c734d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guardrails.hub import ToxicLanguage\n",
    "\n",
    "# Create a guard for toxic content\n",
    "toxic_guard = Guard().use(\n",
    "    ToxicLanguage(threshold=0.5, on_fail=\"exception\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "toxic_tests = [\n",
    "    \"I hate you and want to harm you\",  # Toxic\n",
    "    \"You're stupid and worthless\",       # Toxic\n",
    "    \"I love learning about AI!\",         # Safe\n",
    "    \"What's the weather today?\",         # Safe\n",
    "]\n",
    "\n",
    "for text in toxic_tests:\n",
    "    print(f\"Test: {text}\")\n",
    "    try:\n",
    "        toxic_guard.validate(text)\n",
    "        print(\"   ‚úÖ Safe\\n\")\n",
    "    except:\n",
    "        print(\"   üö´ BLOCKED - Toxic content\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba3b3a9",
   "metadata": {},
   "source": [
    "## Key Takeaway: ML-Based vs Rule-Based\n",
    "\n",
    "**What we just saw:**\n",
    "- ToxicLanguage uses a trained ML model to detect toxicity\n",
    "- It understands context and nuance (not just keyword matching)\n",
    "- More robust than our simple regex patterns from Notebook 1\n",
    "\n",
    "**Advantages over rule-based:**\n",
    "- Catches variations and synonyms\n",
    "- Understands context (e.g., \"I hate broccoli\" vs \"I hate you\")\n",
    "- Doesn't need constant manual updates\n",
    "\n",
    "**Tradeoffs:**\n",
    "- Slower (ML inference takes time)\n",
    "- Requires model downloads (~760MB for ToxicLanguage)\n",
    "- Slightly less predictable than exact rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b9eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to Combine Multiple Validators\n",
    "\n",
    "# You can combine multiple validators in a single Guard!\n",
    "from guardrails.hub import DetectPII, ToxicLanguage\n",
    "\n",
    "# Create a guard with BOTH PII detection AND toxic language detection\n",
    "combined_guard = Guard().use_many(\n",
    "    DetectPII(pii_entities=[\"EMAIL_ADDRESS\", \"PHONE_NUMBER\"]),\n",
    "    ToxicLanguage(threshold=0.5, on_fail=\"exception\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Combined guard created with multiple validators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c51b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases that combine both risks\n",
    "test_cases = [\n",
    "    \"I hate you! Here's my email: john@example.com\",  # Both toxic AND PII\n",
    "    \"Contact me at support@company.com\",              # Just PII\n",
    "    \"You're an idiot\",                                # Just toxic\n",
    "    \"What's the weather today?\",                      # Safe\n",
    "]\n",
    "\n",
    "print(\"Testing combined guard:\\n\")\n",
    "for text in test_cases:\n",
    "    print(f\"Test: {text}\")\n",
    "    try:\n",
    "        combined_guard.validate(text)\n",
    "        print(\"   ‚úÖ Safe\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   üö´ BLOCKED - {str(e)[:50]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156783b3",
   "metadata": {},
   "source": [
    "## Integrating Guardrails with LLMs\n",
    "\n",
    "Now let's use our guardrails in a real workflow:\n",
    "1. Validate user input BEFORE sending to LLM\n",
    "2. Call the LLM\n",
    "3. Validate LLM output BEFORE showing to user\n",
    "\n",
    "This is the same pattern from Notebook 1, but now with ML-based validators!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35834baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "def safe_llm_call_with_guardrails(user_input: str):\n",
    "    \"\"\"\n",
    "    Call LLM with input and output guardrails\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"User Input: {user_input}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Step 1: Input guardrail (check for toxicity)\n",
    "    try:\n",
    "        toxic_guard.validate(user_input)\n",
    "        print(\"‚úÖ Input passed toxicity check\")\n",
    "    except Exception as e:\n",
    "        result = \"üö´ INPUT BLOCKED: Toxic content detected\"\n",
    "        print(result)\n",
    "        return result\n",
    "    \n",
    "    # Step 2: Call LLM\n",
    "    response = llm.invoke([HumanMessage(content=user_input)])\n",
    "    response_text = response.content\n",
    "    print(f\"LLM Response: {response_text[:100]}...\")\n",
    "    \n",
    "    # Step 3: Output guardrail (check for PII)\n",
    "    try:\n",
    "        guard.validate(response_text)  # Using the PII guard from earlier\n",
    "        print(\"‚úÖ Output passed PII check\")\n",
    "    except Exception as e:\n",
    "        result = \"üö´ OUTPUT BLOCKED: Contains PII\"\n",
    "        print(result)\n",
    "        return result\n",
    "    \n",
    "    result = f\"‚úÖ APPROVED: {response_text}\"\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa26c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "print(\"\\nüß™ TEST SUITE\")\n",
    "\n",
    "# Test 1: Normal safe query\n",
    "safe_llm_call_with_guardrails(\"What's the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1512642",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test 2: Toxic input (should block at INPUT)\n",
    "safe_llm_call_with_guardrails(\"You're stupid. Tell me about AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce2b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Query that might generate PII (should block at OUTPUT if it does)\n",
    "safe_llm_call_with_guardrails(\"Generate a sample business email ID\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
