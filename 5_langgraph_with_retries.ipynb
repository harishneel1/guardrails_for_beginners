{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8750f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from guardrails import Guard\n",
    "from guardrails.hub import DetectPII, ToxicLanguage\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5601a2",
   "metadata": {},
   "source": [
    "# Notebook 5: Advanced Guardrail Patterns - Retry Logic\n",
    "\n",
    "## The Problem with Simple Blocking\n",
    "\n",
    "When output guardrails fail, blocking loses valid responses:\n",
    "```\n",
    "User: \"Tell me about our CEO\"\n",
    "LLM: \"John Smith, email: john@company.com\"\n",
    "Output Guardrail: üö´ BLOCKED\n",
    "User sees: \"Sorry, can't show that response\"\n",
    "```\n",
    "\n",
    "The LLM could answer safely‚Äîwe just need to ask again without the problematic content.\n",
    "\n",
    "---\n",
    "\n",
    "## The Solution: Automatic Retry with Feedback\n",
    "\n",
    "1. Detect what failed (PII, toxicity, etc.)\n",
    "2. Ask the LLM to regenerate without that issue\n",
    "3. Try up to N times before giving up\n",
    "\n",
    "**Tradeoffs:**\n",
    "- ‚úÖ Users get answers, LLM learns what to avoid\n",
    "- ‚ùå Adds latency and cost (multiple API calls), no guarantee of success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6801668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create guardrails\n",
    "input_guard = Guard().use(\n",
    "    ToxicLanguage(threshold=0.5, on_fail=\"exception\")\n",
    ")\n",
    "\n",
    "output_guard = Guard().use(\n",
    "    DetectPII(pii_entities=[\"EMAIL_ADDRESS\", \"PHONE_NUMBER\"], on_fail=\"exception\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c57f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"State for our guarded agent\"\"\"\n",
    "    messages: list  # Conversation history\n",
    "    user_input: str  # Current user input\n",
    "    llm_output: str  # LLM's response\n",
    "    input_safe: bool  # Did input pass guardrails?\n",
    "    output_safe: bool  # Did output pass guardrails?\n",
    "    final_response: str  # What we show to the user\n",
    "    retry_count: int  # Track number of retries\n",
    "    feedback: str  # Feedback for LLM on retry\n",
    "\n",
    "print(\"‚úÖ State defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_guardrail_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Check if user input is safe\"\"\"\n",
    "    user_input = state[\"user_input\"]\n",
    "    \n",
    "    print(f\"\\nüîç Checking input: {user_input}\")\n",
    "    \n",
    "    try:\n",
    "        input_guard.validate(user_input)\n",
    "        print(\"‚úÖ Input passed guardrails\")\n",
    "        return {**state, \"input_safe\": True}\n",
    "    except Exception as e:\n",
    "        print(f\"üö´ Input blocked: Toxic content detected\")\n",
    "        return {\n",
    "            **state, \n",
    "            \"input_safe\": False,\n",
    "            \"final_response\": \"I cannot process that request due to inappropriate content.\"\n",
    "        }\n",
    "\n",
    "def llm_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generate LLM response (only if input was safe)\"\"\"\n",
    "    if not state[\"input_safe\"]:\n",
    "        return state\n",
    "    \n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    print(f\"\\nü§ñ Generating LLM response (attempt {retry_count + 1})...\")\n",
    "    \n",
    "    # Build message history\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    # First attempt: use original user input\n",
    "    if retry_count == 0:\n",
    "        messages.append(HumanMessage(content=state[\"user_input\"]))\n",
    "    else:\n",
    "        # Retry: add feedback message\n",
    "        feedback_msg = state.get(\"feedback\", \"Please try again without sensitive information.\")\n",
    "        messages.append(HumanMessage(content=feedback_msg))\n",
    "    \n",
    "    # Call LLM\n",
    "    response = llm.invoke(messages)\n",
    "    llm_output = response.content\n",
    "    \n",
    "    print(f\"LLM said:\\n{'-'*50}\\n{llm_output}\\n{'-'*50}\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"llm_output\": llm_output,\n",
    "        \"messages\": messages + [response]\n",
    "    }\n",
    "\n",
    "def output_guardrail_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Check if LLM output is safe\"\"\"\n",
    "    if not state[\"input_safe\"]:\n",
    "        return state\n",
    "    \n",
    "    llm_output = state[\"llm_output\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    \n",
    "    print(f\"\\nüîç Checking output (attempt {retry_count + 1})...\")\n",
    "    \n",
    "    try:\n",
    "        output_guard.validate(llm_output)\n",
    "        print(\"‚úÖ Output passed guardrails\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"output_safe\": True,\n",
    "            \"final_response\": llm_output\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"üö´ Output blocked: Contains PII\")\n",
    "        \n",
    "        # Give feedback for retry\n",
    "        feedback = (\n",
    "            \"Your previous response contained PII (personal information like email addresses or phone numbers). \"\n",
    "            \"Please rewrite your response without including any personal contact information, \"\n",
    "            \"email addresses, or phone numbers.\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            \"output_safe\": False,\n",
    "            \"retry_count\": retry_count + 1,\n",
    "            \"feedback\": feedback\n",
    "        }\n",
    "\n",
    "def should_retry(state: AgentState) -> str:\n",
    "    \"\"\"Decide whether to retry or end\"\"\"\n",
    "    # If input wasn't safe, end immediately\n",
    "    if not state[\"input_safe\"]:\n",
    "        return \"end\"\n",
    "    \n",
    "    # If output is safe, we're done\n",
    "    if state[\"output_safe\"]:\n",
    "        return \"end\"\n",
    "    \n",
    "    # Check retry limit (max 3 attempts)\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    if retry_count >= 3:\n",
    "        print(f\"‚ö†Ô∏è Max retries ({retry_count}) reached\")\n",
    "        return \"max_retries\"\n",
    "    \n",
    "    print(f\"üîÑ Retrying (attempt {retry_count + 1}/3)...\")\n",
    "    return \"retry\"\n",
    "\n",
    "print(\"‚úÖ Nodes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1327f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"input_check\", input_guardrail_node)\n",
    "workflow.add_node(\"llm\", llm_node)\n",
    "workflow.add_node(\"output_check\", output_guardrail_node)\n",
    "\n",
    "# Define the flow\n",
    "workflow.set_entry_point(\"input_check\")\n",
    "workflow.add_edge(\"input_check\", \"llm\")\n",
    "workflow.add_edge(\"llm\", \"output_check\")\n",
    "\n",
    "# Add conditional edge for retry logic\n",
    "workflow.add_conditional_edges(\n",
    "    \"output_check\",\n",
    "    should_retry,\n",
    "    {\n",
    "        \"retry\": \"llm\",  # Go back to LLM for another attempt\n",
    "        \"max_retries\": END,  # Max retries reached, give up\n",
    "        \"end\": END  # Success or input was unsafe\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"‚úÖ LangGraph agent compiled with retry logic!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e39a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(user_input: str):\n",
    "    \"\"\"Run the agent with a user input\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"USER: {user_input}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    result = app.invoke({\n",
    "        \"user_input\": user_input,\n",
    "        \"messages\": [],\n",
    "        \"input_safe\": False,\n",
    "        \"output_safe\": False,\n",
    "        \"llm_output\": \"\",\n",
    "        \"final_response\": \"\",\n",
    "        \"retry_count\": 0,\n",
    "        \"feedback\": \"\"\n",
    "    })\n",
    "    \n",
    "    # Handle case where max retries reached\n",
    "    if result.get(\"retry_count\", 0) >= 3 and not result.get(\"output_safe\", False):\n",
    "        result[\"final_response\"] = (\n",
    "            \"I couldn't generate a safe response after multiple attempts. \"\n",
    "            \"Please rephrase your question to avoid requesting sensitive information.\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    # print(f\"AGENT: {result['final_response']}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Test cases\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING THE AGENT\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455bddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Normal query (should work)\n",
    "print(\"\\nüìù Test 1: Normal query\")\n",
    "run_agent(\"What's the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45459dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Query that might trigger PII output (will retry)\n",
    "print(\"\\nüìù Test 2: Query that might generate PII\")\n",
    "run_agent(\"Create a sample customer record for John Smith with all his contact details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb7504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Toxic input (should block immediately)\n",
    "print(\"\\nüìù Test 3: Toxic input\")\n",
    "run_agent(\"You are stupid and useless\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
